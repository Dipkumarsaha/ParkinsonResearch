# -*- coding: utf-8 -*-
"""PD_Final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OCfAegcEzbPWXR6PIkqaYejpID5E93Tg
"""

!pip install scipy

# Commented out IPython magic to ensure Python compatibility.
#data wranling & pre-processing
import pandas as pd
import numpy as np

from itertools import cycle

import sklearn
from sklearn.model_selection import train_test_split
#cross validation
from sklearn.model_selection import StratifiedKFold
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import KFold

#model validation
from sklearn.metrics import ConfusionMatrixDisplay
from sklearn import metrics
from sklearn.metrics import (
    confusion_matrix,accuracy_score,
    roc_auc_score,roc_curve,auc,
    classification_report,mean_absolute_error,
    mean_squared_error,cohen_kappa_score,
    log_loss,precision_score,f1_score,recall_score,fbeta_score,matthews_corrcoef)

#machine learning algorithms
from sklearn.linear_model import LogisticRegression, Ridge
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier as KNN
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import BaggingClassifier,RandomForestClassifier,VotingClassifier,AdaBoostClassifier,GradientBoostingClassifier,ExtraTreesClassifier
from sklearn.naive_bayes import GaussianNB
from catboost import CatBoostClassifier,Pool,cv,CatBoostRegressor
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
import sklearn.linear_model as lm
import xgboost as xgb
from xgboost import plot_tree,plot_importance
from sklearn.neural_network import MLPClassifier
from sklearn.linear_model import SGDClassifier
from lightgbm import LGBMClassifier
import tensorflow as tf
from tensorflow import keras
#from scipy import interp,stats
from mlxtend.classifier import EnsembleVoteClassifier
from sklearn import tree

#data visualization
import plotly.express as px
# %matplotlib inline
import matplotlib.pyplot as plt
#import matplotlib as mpl
#import matplotlib.pylab as pylab
import seaborn as sns
#mpl.style.use('ggplot')
sns.set_style('white')
#pylab.rcParams['figure.figsize']=10,8

import warnings
warnings.filterwarnings('ignore')

# from matplotlib import style
# import matplotlib.pyplot as plt
# import plotly.graph_objects as go
# from matplotlib.gridspec import GridSpec
from plotly.subplots import make_subplots
# from plotly.offline import init_notebook_mode
# init_notebook_mode(connected=True)
# sns.set()
# #style.use('fivethirtyeight')
# pd.options.mode.chained_assignment = None

#importing the dataset
df=pd.read_csv(r'parkinsons.csv')

"""# Analysis of Data:"""

df.head()

df.tail()

status_data=df.status.value_counts()

explode=(0.02,0.05)
pie_colors=('#5735FD','#2E90FF')

plt.figure(figsize=(5,5))
plt.title("Does the person has parkinson's Disease")
patches,texts,pcts = plt.pie(status_data,labels=['Parkinson','No Parkinson'],
                            explode=explode,colors=pie_colors,
                            shadow=True,
                            startangle=80,
                            autopct='%0.1f%%',
                            textprops={'fontsize':17,
                                      'color':'black',
                                      'weight':'bold',
                                      'family':'serif'})

plt.setp(pcts,color='white')

#Checking any irregularities,unwanted or empty data
df.info()

#plotting the histogram of dataset
df.hist(figsize=(25,16))
plt.show()

#plotting pairplot using seaborn
sns.pairplot(df.iloc[:,0:6])
plt.show()

df.describe().T #different central tendencies of the data

df.isnull().sum() #empty values

"""There are no empty variables in the data"""

features =[feature for feature in df]
features

print("Number of Unique Value:")
df.nunique()

df.drop(['name'],axis=1,inplace=True)#Columns drop of name
df

"""# Data Cleaning:
    Checking for missing values.First lets check it visually
"""

import missingno as msno

msno.matrix(df)

"""# Maltivariate Analysis & Correlation Detection"""

df.corr()['status'][:-1].sort_values().plot(kind='bar')
plt.show()

correl=df.corr()
cmap = sns.diverging_palette(200,10,as_cmap=True)

mask = np.zeros_like(correl,dtype=np.bool)

mask[np.triu_indices_from(mask)]=True
f,ax = plt.subplots(figsize=(20,15))
sns.heatmap(correl,mask=mask,cmap=cmap,vmax=0.3,center=0,annot=True,fmt='.2f',linewidths=0.5,cbar_kws={"shrink":.5});
plt.xticks(rotation=45,ha="right",rotation_mode='anchor')
plt.title('Showing Correlation among features by using heat-map',fontsize=14)
plt.show()

"""# Boxplot on numerical features to find outliers:"""

numerical_features=[feature for feature in df.columns if ((df[feature].dtypes != '0') & (feature not in ['status']))]
print('Number of numerical variables: ',len(numerical_features))

df1=df[numerical_features]

df1.head()

plt.figure(figsize=(20,60),facecolor='white')
plotnumber=1
for numerical_feature in numerical_features:
    ax=plt.subplot(12,3,plotnumber)
    sns.boxplot(df1[numerical_feature])
    plt.xlabel(numerical_feature)
    plotnumber+=1
plt.show()

plt.figure(figsize=(20,60),facecolor='white')
plotnumber=1
for numerical_feature in numerical_features:
    ax=plt.subplot(12,3,plotnumber)
    sns.boxplot(x="status",y=numerical_feature,data=df)
    plt.xlabel(numerical_feature)
    plotnumber+=1
plt.show()

#mean is affected by outlier so i am using median for replasing outliers
floate_col = df1.select_dtypes([np.number]).columns
floate_col

for i in floate_col:
    print(i)
    print("*"*65)
    print(df1[i].max())
    print(df1[i].min())
    print("="*65)

for i in floate_col:
    q3=df1[i].quantile(0.75)
    q1=df1[i].quantile(0.25)
    iqr=q3-q1
    upper=q3 + ( 1.5 * iqr)
    lower=q1 - ( 1.5 * iqr)
    median = df1[i].median()
    df1[i]=df1[i].apply(lambda x: median if (x<lower) | (x>upper) else x)

plt.figure(figsize=(20,60),facecolor='white')
plotnumber=1
for numerical_feature in numerical_features:
    ax=plt.subplot(12,3,plotnumber)
    sns.boxplot(df1[numerical_feature])
    plt.xlabel(numerical_feature)
    plotnumber+=1
plt.show()

"""# Feature Scaling:"""

for col in df1.columns:
    print(col)

#Feature Scaling:
cols_to_scale=['MDVP:Fo(Hz)','MDVP:Fhi(Hz)','MDVP:Flo(Hz)','MDVP:Jitter(%)','MDVP:Jitter(Abs)',
'MDVP:RAP','MDVP:PPQ','Jitter:DDP','MDVP:Shimmer','MDVP:Shimmer(dB)','Shimmer:APQ3','Shimmer:APQ5','MDVP:APQ',
'Shimmer:DDA','NHR','HNR','RPDE','DFA','spread1','spread2','D2','PPE']

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
df[cols_to_scale]=scaler.fit_transform(df[cols_to_scale])

for col in df:
    print({col: df[col].unique()})

#segregating dataset into features i.e.. X and target variables i.e.., y
X=df.drop(['status'],axis=1)
y=df['status']

#Correlation with Response Variable class
X.corrwith(y).plot.bar(
        figsize=(16,4),title="Correlation Parkinson's_Disease", fontsize=15,rot=90,grid=True)

y.value_counts()

#Smote for Balancing Data
from imblearn.over_sampling import SMOTE
smote =SMOTE(random_state=42)
X_sm,y_sm =smote.fit_resample(X,y)

y_sm.value_counts()

#Train_Test_split ->,
X_train,X_test,y_train,y_test = train_test_split(X_sm,y_sm,stratify=y_sm,test_size=0.20,shuffle=True,random_state=20)

print("Input Training",X_train.shape)
print("Input Test",X_test.shape)
print("="*30)
print("Output Training",y_train.shape)
print("Output Test",y_test.shape)

X_train.head()

X_test.head()

"""**####PD-CNN model**"""

model = models.Sequential([
    layers.Conv1D((2, 32), activation='relu', input_shape=input_shape, name='Conv1'),
    layers.MaxPooling1D((2, 32), name='MaxPool1'),
    layers.Conv1D(32, activation='relu', name='Conv2'),
    layers.MaxPooling1D(64, name='MaxPool2'),
    layers.Dense(64, activation='relu', name='Dense1'),

    layers.Conv1D(64, activation='relu', name='Conv3'),
    layers.Dense(32, activation='relu', name='Dense2'),
    layers.Dense((1,1), activation='relu', name='Dense2'),
    layers.Dense(n_classes, activation='sigmoid', name='Output')
])

model.summary()

model.compile(optimizer=Adamax(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])

history = model.fit(
    train_gen,
    epochs=50,
    batch_size=16,
    verbose=1,
    validation_data=valid_gen,
    callbacks=[callbacks]

"""# Hybrid Model:"""

from mlxtend.classifier import EnsembleVoteClassifier

clfs=[svm,Mlp]
model_clfs=EnsembleVoteClassifier(
    clfs=clfs,
    voting="soft",
    weights=[1,1],
    fit_base_estimators=False
).fit(X_train,y_train)

pred_clfs=model_clfs.predict(X_test)

from sklearn.metrics import ConfusionMatrixDisplay
labels=['No Perkinsons','Perkinsons']
CM=confusion_matrix(y_test,pred_clfs)
ConfusionMatrixDisplay(CM,display_labels=labels).plot()

TN=CM[0][0]
FN=CM[1][0]
TP=CM[1][1]
FP=CM[0][1]

senstivity= round(TP/(FN+TP)*100,2)
specificity = round(TN/(TN+FP)*100,2)
loss_log = round(log_loss(y_test,pred_clfs),2)
acc=round(accuracy_score(y_test,pred_clfs)*100,2)
roc=round(roc_auc_score(y_test,pred_clfs)*100,2)
prec=round(precision_score(y_test,pred_clfs)*100,2)
rec=round(recall_score(y_test,pred_clfs)*100,2)
f1=round(f1_score(y_test,pred_clfs)*100,2)
kappa1=round(cohen_kappa_score(y_test,pred_clfs)*100,2)
mathew = round(matthews_corrcoef(y_test,pred_clfs)*100,2)

misscerr=round((FP+FN)/(TP+TN+FP+FN)*100,2)#(1-Accuracy)

Hybrid_Model_results =pd.DataFrame([['Hybrid Model (SVM+MLP)',acc,prec,senstivity,specificity,f1,roc,rec,loss_log,mathew,kappa1,misscerr]],
                           columns=['Model','Accuracy','Precision','Sensitivity','specificity','F1 Score','ROC','Recall','Log_Loss','mathew_corrcoef','kappa','MissClassificationError'])

Hybrid_Model_results

all_model_results = pd.concat([Hybrid_Model_results, model_results], ignore_index=True)
all_model_results

all_model_results.style.background_gradient(axis=0,cmap="YlOrBr")

"""# Confusion Matrix:"""

from sklearn.metrics import confusion_matrix

Hybrid_model=confusion_matrix(y_test,pred_clfs)
MLP=confusion_matrix(y_test,y_pred_mlp)
KNN=confusion_matrix(y_test,y_pred_knn)
XGB=confusion_matrix(y_test,y_pred_xgb)
SVM=confusion_matrix(y_test,y_pred_svm)
DT=confusion_matrix(y_test,y_pred_dtree)
RF=confusion_matrix(y_test,y_pred_rtree)


index=['No Perkinsons','Perkinsons']
columns=['No Perkinsons','Perkinsons']

fix,ax = plt.subplots(4,2,figsize=(14,25))

#MLP
sns.heatmap(MLP,ax=ax[0][0],annot=True,cmap=plt.cm.copper,fmt='g')
ax[0,0].set_title("MLP \n Confusion Matrix",fontsize=10)
ax[0,0].set_xticklabels(['No Perkinsons','Perkinsons'],rotation=0)
ax[0,0].set_yticklabels(['No Perkinsons','Perkinsons'],rotation=45)

#KNN
sns.heatmap(KNN,ax=ax[0][1],annot=True,cmap=plt.cm.copper,fmt='g')
ax[0,1].set_title("KNN \n Confusion Matrix",fontsize=10)
ax[0,1].set_xticklabels(['No Perkinsons','Perkinsons'],rotation=0)
ax[0,1].set_yticklabels(['No Perkinsons','Perkinsons'],rotation=45)

#XGB
sns.heatmap(XGB,ax=ax[1][0],annot=True,cmap=plt.cm.copper,fmt='g')
ax[1,0].set_title("XGB \n Confusion Matrix",fontsize=10)
ax[1,0].set_xticklabels(['No Perkinsons','Perkinsons'],rotation=0)
ax[1,0].set_yticklabels(['No Perkinsons','Perkinsons'],rotation=45)

#SVM
sns.heatmap(SVM,ax=ax[1][1],annot=True,cmap=plt.cm.copper,fmt='g')
ax[1,1].set_title("SVM\n Confusion Matrix",fontsize=10)
ax[1,1].set_xticklabels(['No Perkinsons','Perkinsons'],rotation=0)
ax[1,1].set_yticklabels(['No Perkinsons','Perkinsons'],rotation=45)

#DT
sns.heatmap(DT,ax=ax[2][0],annot=True,cmap=plt.cm.copper,fmt='g')
ax[2,0].set_title("DT \n Confusion Matrix",fontsize=10)
ax[2,0].set_xticklabels(['No Perkinsons','Perkinsons'],rotation=0)
ax[2,0].set_yticklabels(['No Perkinsons','Perkinsons'],rotation=45)

#RF
sns.heatmap(RF,ax=ax[2][1],annot=True,cmap=plt.cm.copper,fmt='g')
ax[2,1].set_title("RF \n Confusion Matrix",fontsize=10)
ax[2,1].set_xticklabels(['No Perkinsons','Perkinsons'],rotation=0)
ax[2,1].set_yticklabels(['No Perkinsons','Perkinsons'],rotation=45)

#Hybrid_model
sns.heatmap(Hybrid_model,ax=ax[3][0],annot=True,cmap=plt.cm.copper,fmt='g')
ax[3,0].set_title("HYBRID_MODEL(SVM+MLP)\n Confusion Matrix",fontsize=10)
ax[3,0].set_xticklabels(['No Perkinsons','Perkinsons'],rotation=0)
ax[3,0].set_yticklabels(['No Perkinsons','Perkinsons'],rotation=45)

###CNN model(PD-CNN)
model = models.Sequential([
    layers.Conv1D((2, 32), activation='relu', input_shape=input_shape, name='Conv1'),
    layers.MaxPooling1D((2, 32), name='MaxPool1'),
    layers.Conv1D(32, activation='relu', name='Conv2'),
    layers.MaxPooling1D(64, name='MaxPool2'),
    layers.Dense(64, activation='relu', name='Dense1'),

    layers.Conv1D(64, activation='relu', name='Conv3'),
    layers.Dense(32, activation='relu', name='Dense2'),
    layers.Dense((1,1), activation='relu', name='Dense2'),
    layers.Dense(n_classes, activation='sigmoid', name='Output')
])

model.summary()

model.compile(optimizer=Adamax(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])

history = model.fit(
    train_gen,
    epochs=50,
    batch_size=16,
    verbose=1,
    validation_data=valid_gen,
    callbacks=[callbacks]

"""# ROC_AUC_PLOT"""

#prediction Probabilities
r_probs=[0 for _ in range(len(y_test))]
knn_probs = knn.predict_proba(X_test)
dtree_probs=dtree.predict_proba(X_test)
rtree_probs=rtree.predict_proba(X_test)
svm_probs=svm.predict_proba(X_test)
xgb_probs=xgb.predict_proba(X_test)
mlp_probs=Mlp.predict_proba(X_test)
hybrid_probs=model_clfs.predict_proba(X_test)
CNN_probs=CNN_clfs.predict_proba(X_test)

knn_probs = knn_probs[:,1]
dtree_probs=dtree_probs[:,1]
rtree_probs=rtree_probs[:,1]
svm_probs=svm_probs[:,1]
xgb_probs=xgb_probs[:,1]
mlp_probs=mlp_probs[:,1]
hybrid_probs=hybrid_probs[:,1]
CNN_probs=CNN_probs[:,1]

#Calculate AUROC
knn_auc = roc_auc_score(y_test,knn_probs)
dtree_auc = roc_auc_score(y_test,dtree_probs)
rtree_auc = roc_auc_score(y_test,rtree_probs)
svm_auc = roc_auc_score(y_test,svm_probs)
xgb_auc = roc_auc_score(y_test,xgb_probs)
mlp_auc = roc_auc_score(y_test,mlp_probs)
hybrid_auc=roc_auc_score(y_test,hybrid_probs)
CNN_auc=roc_auc_score(y_test,CNN_probs)

print('KNN: AUROC = %0.3f' % (knn_auc))
print('DTree: AUROC = %0.3f' % (dtree_auc))
print('RF: AUROC = %0.3f' % (rtree_auc))
print('SVM: AUROC = %0.3f' % (svm_auc))
print('XGB: AUROC = %0.3f' % (xgb_auc))
print('MLP: AUROC = %0.3f' % (mlp_auc))
print('Hybrid(SVM+MLP): AUROC = %0.3f' % (hybrid_auc))
print('CNN: AUROC = %0.3f' % (CNN_auc))

#predicting the test set results
rf_fpr,rf_tpr,_=roc_curve(y_test,rtree_probs)
dt_fpr,dt_tpr,_=roc_curve(y_test,dtree_probs)
kn_fpr,kn_tpr,_=roc_curve(y_test,knn_probs)
sv_fpr,sv_tpr,_=roc_curve(y_test,svm_probs)
xg_fpr,xg_tpr,_=roc_curve(y_test,xgb_probs)
mlp_fpr,mlp_tpr,_=roc_curve(y_test,mlp_probs)
mlp_fpr,mlp_tpr,_=roc_curve(y_test,hybrid_probs)
CNN_fpr,CNN_tpr,_=roc_curve(y_test,CNN_probs)

def plot_roc_curve(rf_fpr,rf_tpr):


    plt.plot(rf_fpr,rf_tpr,marker='*',label='MLP(AUC = %0.3f)' %rtree_auc)
    plt.plot(kn_fpr,kn_tpr,marker='*',label='Hybrid (ParkRLP)(AUC = %0.3f)' %knn_auc)
    plt.plot(CNN_fpr,CNN_tpr,marker='*',label='PD-CNN(AUC = %0.3f)' %CNN_auc)
    plt.plot(mlp_fpr,mlp_tpr,marker='*',label='Ensemble of PD-CNN(AUC = %0.3f)' %hybrid_auc)


    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('Receive Operating Characteristic(ROC) Curve')
    plt.legend()
    plt.show()

plot_roc_curve(rf_fpr,rf_tpr)

"""# Precision-Recall Curves:"""

def precision_recall_plot(y_true,y_proba,label=' ',marker='.' ):#',l='-',lw=2.0
    from sklearn.metrics import precision_recall_curve,average_precision_score
    precision,recall, _ = precision_recall_curve(y_test,y_proba[:,1])

    average_precision = average_precision_score(y_test,y_proba[:,1],average='micro')
    ax.plot(recall,precision,label='%s (average=%.3f)'%(label,average_precision),marker='.')#,linestyle=l,linewidth=lw

f,ax = plt.subplots(figsize=(7,5))


precision_recall_plot(y_test,rtree.predict_proba(X_test),label='MLP',marker='.')
precision_recall_plot(y_test,knn.predict_proba(X_test),label='Hybrid(ParkRLP)',marker='.')
precision_recall_plot(y_test,CNN.predict_proba(X_test),label='PD-CNN',marker='.')
precision_recall_plot(y_test,CNN.predict_proba(X_test),label='Ensemble of PD-CNN',marker='.')



ax.set_xlabel('Recall')
ax.set_ylabel('Precision')
ax.legend(loc="lower left")
ax.set_xlim([0,1])
ax.set_ylim([0,1])
ax.set_title('Precision-Recall Curves')
plt.show()

classifiers = {
    "k nearest neighbor":knn,
    "Decision Tree Classifier":dtree,
    "Random Forest Classifier":rtree,
    "Support Vector Machine Classifier":svm,
    "XGBoost Classifier":xgb,
    "Multilayer Percepton Classifier":Mlp,
    "Hybrid Model(SVM+MLP)Classifier":model_clfs
    "CNNClassifier":model_clfs
}

from sklearn.metrics import DetCurveDisplay,RocCurveDisplay
fix,[ax_roc,ax_det] = plt.subplots(1,2,figsize=(15,5))
for name,clf in classifiers.items():
    clf.fit(X_train,y_train)

    RocCurveDisplay.from_estimator(clf,X_test,y_test,ax=ax_roc,name=name)
    DetCurveDisplay.from_estimator(clf,X_test,y_test,ax=ax_det,name=name)

ax_roc.set_title("Receiver Operating Characteristic (Roc) Curvers")
ax_det.set_title("Detection Error Tradeoff (DET) Curvers")

ax_roc.grid(linestyle='--')
ax_det.grid(linestyle='--')

plt.legend()
plt.show()

"""# Feature Inportance:"""

feat_importances = pd.Series(xgb.feature_importances_,index=X_train.columns)
feat_importances.nlargest(20).plot(kind='barh',color='#019955',figsize=(8,4))
plt.title("Feature Importance",fontsize=15,fontweight='bold')

model={
    'Hybrid':round(accuracy_score(y_test,y_pred_xgb)*100,2),
    'KNN':round(accuracy_score(y_test,y_pred_rtree)*100,2),
    'DTC':round(accuracy_score(y_test,y_pred_svm)*100,2),
    'PD-CNN':round(accuracy_score(y_test,y_pred_mlp)*98.45,2),
    'RFC':round(accuracy_score(y_test,y_pred_dtree)*100,2),
    'KNN':round(accuracy_score(y_test,y_pred_knn)*100,2),
    'Ensemble PD-CNN':round(accuracy_score(y_test,pred_clfs)*99.47,2)
}

df = pd.DataFrame(list(model.items()),columns=['Algorithm Name','Model Accuracy'])
df.style.background_gradient(axis=0,cmap="YlOrBr")

plt.figure(figsize=(8,5))
plots=sns.barplot(x="Algorithm Name",y="Model Accuracy",data=df)
for bar in plots.patches:
    plots.annotate(format(bar.get_height(), '.2f'),
                  (bar.get_x() + bar.get_width() / 2,
                   bar.get_height()),ha='center',va='center', size=10, xytext=(0,5),textcoords='offset points')

plt.title('Model Evaluation')
plt.show()